import asyncio
import os
import sys

# --- å…³é”®è®¾ç½®ï¼šé€‚é… src ç›®å½•ç»“æ„ ---
# è¿™ä¸€æ­¥ç¡®ä¿å³ä½¿æ²¡æœ‰è¿è¡Œ 'pip install -e .' ä¹Ÿèƒ½æ‰¾åˆ° goose åŒ…
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

print(f"ğŸ“‚ Added source path: {src_path}")

# --- å¯¼å…¥æ¨¡å— (åŸºäºæœ€æ–°çš„ goose åŒ…ç»“æ„) ---
from goose.session import SessionManager
from goose.conversation.message import Message, TextContent
from goose.model import ModelConfig
from goose.providers import OpenAIProvider

# --- é…ç½®åŒºåŸŸ (è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹) ---
# vLLM / Qwen / Ollama é…ç½®
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"
API_KEY = "sk-climzomnsicqdepumaymoshvgviaggcgounvovaqglltepkd"
API_BASE = "https://api.siliconflow.cn/v1"
# API_BASE = "http://192.168.10.180:8088/v1/" 
# API_KEY = "vllm"
# MODEL_NAME = "qwen3_vl" 

async def main():
    print("\nğŸš€ Starting Goose-Py LLM Integration Test (Src Layout)\n")

    # 1. åˆå§‹åŒ–æ•°æ®åº“ (SessionStorage)
    print("Step 1: Initializing Database...")
    # è¿™ä¼šè‡ªåŠ¨è¿è¡Œ migrations åˆ›å»ºè¡¨
    await SessionManager.get_storage()
    print("âœ… Database initialized.")

    # 2. åˆ›å»ºæ–°ä¼šè¯
    print("\nStep 2: Creating Session...")
    session = await SessionManager.create_session(name="Integration Test Session")
    print(f"âœ… Session Created: {session.id} (Type: {session.session_type})")

    # 3. æ„é€ å¹¶å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯
    user_query = "è¯·ç”¨ Python å†™ä¸€ä¸ª Hello Worldï¼Œå¹¶ç®€å•è§£é‡Šä¸€ä¸‹ã€‚"
    print(f"\nStep 3: User sends: '{user_query}'")
    
    user_msg = Message.user(user_query)
    await SessionManager.add_message(session.id, user_msg)
    print("âœ… User message saved to DB.")

    # 4. åˆå§‹åŒ–æ¨¡å‹æä¾›è€… (Provider)
    print(f"\nStep 4: Connecting to Provider ({MODEL_NAME})...")
    config = ModelConfig(
        model_name=MODEL_NAME, 
        temperature=0.7,
        max_tokens=1024
    )
    
    provider = OpenAIProvider(
        model_config=config,
        base_url=API_BASE,
        api_key=API_KEY
    )

    # 5. è·å–å†å²è®°å½• (ç”¨äºå‘é€ç»™ LLM)
    history = await SessionManager.get_messages(session.id)
    system_prompt = "You are a professional coding assistant named Goose."

    # 6. æµå¼è°ƒç”¨ LLM å¹¶å®æ—¶è¾“å‡º
    print("\nStep 5: Streaming Response...")
    print("-" * 50)
    
    full_response_text = ""
    token_usage = None
    first_token_received = False

    try:
        # è®°å½•å¼€å§‹æ—¶é—´ï¼Œç”¨äºæ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹å¤ªæ…¢
        import time
        start_time = time.time()

        async for msg, usage in provider.stream(system_prompt, history):
            # [è°ƒè¯•] å¦‚æœè¶…è¿‡5ç§’æ²¡ååº”ï¼Œæ‰“å°ç­‰å¾…æç¤º
            if not first_token_received and (time.time() - start_time > 5):
                print("(Waiting for model prefill...)...", end="\n", flush=True)
                start_time = time.time() # é‡ç½®é¿å…é‡å¤æ‰“å°

            # 1. å¤„ç† Token ç»Ÿè®¡ (é€šå¸¸åœ¨æœ€åï¼Œä½†ä¹Ÿå¯èƒ½ä¼´éšæ¶ˆæ¯)
            if usage:
                token_usage = usage
                # å¦‚æœæ˜¯çº¯ Usage æ¶ˆæ¯ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
                if not msg: 
                    continue

            # 2. å¤„ç†æ¶ˆæ¯å†…å®¹
            if msg and msg.content:
                # æ ‡è®°å·²æ”¶åˆ°é¦–å­—
                first_token_received = True
                
                # è·å–ç¬¬ä¸€ä¸ªå†…å®¹å—
                content_item = msg.content[0]
                
                # [è°ƒè¯•] å¦‚æœç±»å‹ä¸å¯¹ï¼Œæ‰“å°ç±»å‹åä»¥ä¾¿æ’æŸ¥
                if not isinstance(content_item, TextContent):
                    # å¯èƒ½æ˜¯ ToolRequest æˆ– Thinkingï¼Œæ‰“å°å‡ºæ¥çœ‹çœ‹
                    print(f"\n[Debug: Non-Text Content: {type(content_item).__name__}]", end="\n", flush=True)
                    continue

                # æ­£å¸¸æ‰“å°æ–‡æœ¬
                chunk = content_item.text
                if chunk:
                    # âœ… ä¿®å¤ï¼šåªä¿ç•™è¿™ä¸€ä¸ª printï¼Œå»æ‰åŸæ¥çš„ç¬¬äºŒä¸ª print(chunk, flush=True)
                    print(chunk, end="", flush=True)
                    full_response_text += chunk
            
    except Exception as e:
        import traceback
        print(f"\n\nâŒ Error during streaming: {e}")
        traceback.print_exc() # æ‰“å°å®Œæ•´å †æ ˆ
        
        if "Connection" in str(e):
            print("Tip: Check if your vLLM server URL is correct and accessible.")
        
        await SessionManager.shutdown()
        return

    # å¦‚æœå¾ªç¯ç»“æŸäº† full_response_text è¿˜æ˜¯ç©ºçš„
    if not full_response_text:
        print("\nâš ï¸ Warning: Stream finished but no text was collected.")
        if token_usage:
            print(f"   (But Usage was received: {token_usage.usage})")
        else:
            print("   (No data received from provider)")

    print("\n" + "-" * 50)

    # 7. å­˜å‚¨ AI å›å¤
    if full_response_text:
        print("\nStep 6: Saving Assistant Response...")
        ai_msg = Message.assistant(full_response_text)
        await SessionManager.add_message(session.id, ai_msg)
        print("âœ… AI response saved.")
    
    # 8. éªŒè¯ä¸ç»Ÿè®¡
    print("\nStep 7: Verification")
    if token_usage:
        print(f"ğŸ“Š Usage Stats: Input={token_usage.usage.input_tokens}, Output={token_usage.usage.output_tokens}")
    
    # éªŒè¯æ•°æ®åº“ä¸­çš„æ¶ˆæ¯æ•°é‡
    stored_msgs = await SessionManager.get_messages(session.id)
    print(f"ğŸ” Messages in DB: {len(stored_msgs)} (Expected >= 2)")
    
    # 9. æ¸…ç†èµ„æº
    await SessionManager.shutdown()
    print("\nğŸ‰ Test Completed Successfully!")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nTest interrupted by user.")