import asyncio
import os
import sys
import shutil
import logging
import json
from typing import Dict, Any, List

# --- 1. ç¯å¢ƒé…ç½® ---
# ç¡®ä¿èƒ½å¯¼å…¥ goose åŒ…
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# æ ¸å¿ƒä¾èµ–
from goose.persistence import SQLiteBackend, PersistenceManager
from goose.session import SessionManager
from goose.workflow.graph import Graph
from goose.workflow.scheduler import WorkflowScheduler
from goose.workflow.repository import WorkflowRepository, register_workflow_schemas
from goose.session.repository import register_session_schemas
from goose.components.registry import component_registry

# ç»„ä»¶ä¾èµ–
from goose.components.buildins.llm import LLMComponent, LLMConfig, OutputDefinition
from goose.components.buildins.code import CodeRunner, CodeConfig, InputMapping
from goose.components.buildins.control import SelectorComponent, SelectorConfig, ConditionBranch
from goose.components.buildins.basic import StartComponent,EndComponent,StartConfig,EndConfig
from goose.providers.base import Provider,ProviderUsage,ProviderFactory
from goose.conversation import Message

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger("test_coze_full")

TEST_DB_PATH = "./temp_test_data/coze_full_workflow.db"



# ä¸ºäº†ä¸ä¿®æ”¹æºç ï¼Œæˆ‘ä»¬è¿™é‡Œä½¿ç”¨ unittest.mock æ¥ patch ProviderFactory
from unittest.mock import patch

# ==========================================
# 3. ç¼–æ’å·¥ä½œæµ (Graph Construction)
# ==========================================

def build_sentiment_graph() -> Graph:
    graph = Graph()

    # --- Node 1: Start (FunctionNode) ---
    # ä½œç”¨ï¼šé€ä¼ ç”¨æˆ·è¾“å…¥ï¼Œä½œä¸º Workflow çš„å…¥å£
    graph.add_node("start_node", StartComponent())

    # --- Node 2: LLM (LLMComponent) ---
    # ä½œç”¨ï¼šåˆ†ææƒ…æ„Ÿå¹¶æ‰“åˆ†
    llm_config = LLMConfig(
        model="mock-gpt",
        prompt="Analyze sentiment: {{ start_node.user_text }}",
        response_format="json_object",
        output_definitions=[
            OutputDefinition(name="score", type="number", description="Sentiment score (0-100)"),
            OutputDefinition(name="reason", type="string", description="Reasoning")
        ]
    )
    
    # å®ä¾‹åŒ–ç»„ä»¶å¹¶ç»‘å®šé…ç½®
    llm_comp = LLMComponent()
    llm_comp.config = llm_config.model_dump() # æ¨¡æ‹Ÿ Converter çš„è¡Œä¸º
    
    # ç»‘å®šè¾“å…¥æ˜ å°„: input_text æ¥è‡ª start_node
    # æ³¨æ„ï¼šæˆ‘ä»¬ä¹‹å‰çº¦å®š Scheduler ä¼šè¯»å– getattr(node, "inputs")
    setattr(llm_comp, "inputs", {"user_text": "{{ start_node.user_text }}"})
    
    graph.add_node("llm_node", llm_comp)

    # --- Node 3: Code (CodeRunner) ---
    # ä½œç”¨ï¼šå¤„ç† JSON æ•°æ® (è™½ç„¶ LLM å·²ç»è¾“å‡ºäº† JSONï¼Œè¿™é‡Œæ¼”ç¤º Code ç»„ä»¶çš„æ•°æ®å¤„ç†èƒ½åŠ›)
    code_config = CodeConfig(
        code="""
def main(args):
    score = args.get('score', 0)
    # ç®€å•çš„ä¸šåŠ¡é€»è¾‘ï¼šå½’ä¸€åŒ–æˆ–åŠ æƒ
    final_score = int(score)
    return {"final_score": final_score, "status": "processed"}
""",
        input_parameters=[
            InputMapping(name="score", value="{{ llm_node.score }}")
        ]
    )
    code_comp = CodeRunner()
    code_comp.config = code_config.model_dump()
    # CodeRunner çš„ inputs é€šå¸¸æ˜¯ç©ºçš„ï¼Œå› ä¸º input_parameters è´Ÿè´£äº†æ˜ å°„ï¼Œ
    # ä½†ä¸ºäº†è§¦å‘ TemplateRendererï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ä¸Šä¸‹æ–‡ã€‚
    # å®é™…ä¸Š CodeRunner.execute å†…éƒ¨ä¼šè§£æ input_parametersã€‚
    setattr(code_comp, "inputs", {}) 
    graph.add_node("code_node", code_comp)

    # --- Node 4: Switch (SelectorComponent) ---
    # ä½œç”¨ï¼šè·¯ç”±åˆ†å‘
    switch_config = SelectorConfig(
        conditions=[
            # å¦‚æœåˆ†æ•° > 60ï¼Œèµ° "high_score" å¥æŸ„
            ConditionBranch(expression="{{ score > 60 }}", target_handle="high_score"),
        ],
        default_handle="low_score"
    )
    switch_comp = SelectorComponent()
    switch_comp.config = switch_config.model_dump()
    # æ³¨å…¥å˜é‡ä¾›è¡¨è¾¾å¼ä½¿ç”¨
    setattr(switch_comp, "inputs", {"score": "{{ code_node.final_score }}"})
    graph.add_node("switch_node", switch_comp)

    # --- Node 5: End Positive ---
    graph.add_node("end_happy", FunctionNode(
        func=lambda **k: {"result": "ğŸ˜Š Positive Vibe!", "details": k},
        name="End Happy"
    ))
    # è¾“å…¥æ˜ å°„ï¼šæ¥æ”¶ Code çš„å¤„ç†ç»“æœ
    graph.nodes["end_happy"].inputs = {"data": "{{ code_node.final_score }}"}

    # --- Node 6: End Negative ---
    graph.add_node("end_sad", FunctionNode(
        func=lambda **k: {"result": "ğŸ˜” Needs Improvement", "details": k},
        name="End Sad"
    ))
    graph.nodes["end_sad"].inputs = {"data": "{{ code_node.final_score }}"}


    # --- Wiring (è¿çº¿) ---
    
    # 1. çº¿æ€§æµ
    graph.add_edge("start_node", "llm_node")
    graph.add_edge("llm_node", "code_node")
    graph.add_edge("code_node", "switch_node")

    # 2. æ¡ä»¶åˆ†æ”¯æµ (Switch è¾“å‡º)
    # å¥æŸ„ "high_score" -> Happy
    graph.add_edge("switch_node", "end_happy", source_handle="high_score")
    # å¥æŸ„ "low_score" -> Sad
    graph.add_edge("switch_node", "end_sad", source_handle="low_score")

    graph.set_entry_point("start_node")
    
    return graph

# ==========================================
# 4. æµ‹è¯•ä¸»ç¨‹åº
# ==========================================

async def setup_env():
    if os.path.exists("./temp_test_data"):
        shutil.rmtree("./temp_test_data")
    os.makedirs("./temp_test_data", exist_ok=True)
    print("ğŸ§¹ Environment cleaned.")

async def main():
    await setup_env()
    print("\nğŸš€ Starting Coze-like Full Workflow Test...\n")

    # 1. åˆå§‹åŒ–æŒä¹…å±‚
    backend = SQLiteBackend(TEST_DB_PATH)
    pm = PersistenceManager.initialize(backend)
    register_workflow_schemas()
    register_session_schemas()
    await pm.boot()

    # 2. æ„å»ºå›¾
    graph = build_sentiment_graph()
    
    # 3. åˆå§‹åŒ–è°ƒåº¦å™¨
    repo = WorkflowRepository()
    scheduler = WorkflowScheduler(graph, checkpointer=repo)

    # --- åœºæ™¯ A: é«˜åˆ†æƒ…å†µ (Mock Score = 85) ---
    print("\nğŸ¬ [Scenario A] Testing Positive Flow (Score=85)...")
    
    mock_high = MockLLMProvider({"score": 85, "reason": "Very happy text"})
    
    # ä½¿ç”¨ Patch æ‹¦æˆª ProviderFactory.createï¼Œè¿”å›æˆ‘ä»¬çš„ Mock Provider
    with patch("goose.providers.factory.ProviderFactory.create", return_value=mock_high):
        
        input_data = {"user_text": "I love coding with Goose!"}
        run_id_a = None
        
        async for event in scheduler.run(input_data):
            if event.type == "workflow_started":
                run_id_a = event.session_id
                print(f"   ğŸ”¹ Session Started: {run_id_a}")
            elif event.type == "node_finished":
                # æ˜¾ç¤ºç®€ç•¥æ—¥å¿—
                out_str = str(event.output_data)[:50] + "..." if event.output_data else "None"
                print(f"   âœ… Node [{event.node_id}] -> {out_str}")
            elif event.type == "workflow_completed":
                print(f"   ğŸ‰ Workflow Completed: {event.final_output}")
                
                # æ–­è¨€ç»“æœ
                assert event.final_output["result"] == "ğŸ˜Š Positive Vibe!"
                print("   âœ… Assertion Passed: Correctly routed to Happy End.")

        # éªŒè¯æ•°æ®åº“çŠ¶æ€
        state = await repo.load_checkpoint(run_id_a)
        assert state.status == "completed"
        # éªŒè¯é˜Ÿåˆ—æ˜¯å¦æ¸…ç©º (æ–°ç‰¹æ€§éªŒè¯)
        assert isinstance(state.execution_queue, list)
        assert len(state.execution_queue) == 0
        print("   âœ… DB Persistence Verified.")


    # --- åœºæ™¯ B: ä½åˆ†æƒ…å†µ (Mock Score = 40) ---
    print("\nğŸ¬ [Scenario B] Testing Negative Flow (Score=40)...")
    
    mock_low = MockLLMProvider({"score": 40, "reason": "Sad text"})
    
    with patch("goose.providers.factory.ProviderFactory.create", return_value=mock_low):
        
        input_data = {"user_text": "Debugging is frustrating."}
        
        async for event in scheduler.run(input_data):
            if event.type == "workflow_completed":
                print(f"   ğŸ‰ Workflow Completed: {event.final_output}")
                
                # æ–­è¨€ç»“æœ
                assert event.final_output["result"] == "ğŸ˜” Needs Improvement"
                print("   âœ… Assertion Passed: Correctly routed to Sad End.")

    # æ¸…ç†
    await PersistenceManager.get_instance().shutdown()
    print("\nâœ¨ All Full-Workflow Scenarios Passed!")

if __name__ == "__main__":
    asyncio.run(main())