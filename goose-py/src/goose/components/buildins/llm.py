import json
import re
import logging
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field, ConfigDict

from goose.components.base import Component
from goose.toolkit import tool_registry, ToolSourceType, ToolDefinition
from goose.workflow.context import WorkflowContext
from goose.utils.template import TemplateRenderer
from goose.conversation import Message, Role, TextContent
from goose.components.registry import register_component
from goose.types import NodeTypes
from goose.events.types import SystemEvents  # å¼•å…¥ç³»ç»Ÿäº‹ä»¶

logger = logging.getLogger("goose.component.llm")

# ==========================================
# Schema Definition
# ==========================================

class OutputDefinition(BaseModel):
    name: str
    type: str = "string"
    description: Optional[str] = None

class LLMConfig(BaseModel):
    # --- æ¨¡å‹é…ç½® ---
    model: str = Field(..., description="æ¨¡å‹èµ„æºID (e.g. sys.model.gpt4o)")
    id: Optional[str] = Field(None, description="è¿è¡Œæ—¶æ³¨å…¥çš„èŠ‚ç‚¹ ID")
    # --- æç¤ºè¯ ---
    prompt: str = Field(..., description="ç”¨æˆ·æç¤ºè¯ (æ”¯æŒ {{var}})")
    system_prompt: str = Field("", description="ç³»ç»Ÿæç¤ºè¯ (æ”¯æŒ {{var}})")
    
    # --- å·¥å…·ä¸å‚æ•° ---
    tools: List[str] = Field(default_factory=list, description="æŒ‚è½½çš„å·¥å…· ID åˆ—è¡¨")
    
    # --- è¾“å‡ºæ§åˆ¶ ---
    response_format: str = Field("text", description="è¾“å‡ºæ¨¡å¼: text æˆ– json_object")
    output_definitions: List[OutputDefinition] = Field(default_factory=list, description="è¾“å‡ºå˜é‡å®šä¹‰")
    
    # --- é«˜çº§å‚æ•° ---
    temperature: float = 0.7
    max_tokens: int = 4096
    max_iterations: int = 5  # ReAct æœ€å¤§å¾ªç¯æ¬¡æ•°
    
    model_config = ConfigDict(extra='allow')

# ==========================================
# LLM Component Implementation
# ==========================================

@register_component(
    name=NodeTypes.LLM,
    group="AI",
    label="å¤§è¯­è¨€æ¨¡å‹",
    description="æ‰§è¡Œå¯¹è¯ã€å·¥å…·è°ƒç”¨åŠç»“æ„åŒ–è¾“å‡º",
    icon="cpu",
    author="System",
    version="1.0.0",
    config_model=LLMConfig
)
class LLMComponent(Component):
    async def execute(
        self, 
        inputs: Dict[str, Any], 
        config: LLMConfig, 
        context: WorkflowContext
    ) -> Dict[str, Any]:
        
        # 1. [å‡†å¤‡] å·¥å…·å®šä¹‰
        tool_defs = []
        openai_tools = []
        
        if config.tools:
            for tool_id in config.tools:
                t_def = tool_registry.get_meta(tool_id)
                if t_def:
                    tool_defs.append(t_def)
                    # è½¬æ¢å·¥å…·å®šä¹‰æ ¼å¼
                    openai_tools.append(self._to_openai_tool(t_def))
                else:
                    logger.warning(f"Tool not found: {tool_id}")

        # 2. [å‡†å¤‡] æ¨¡å‹ Provider
        # ä»èµ„æºç®¡ç†å™¨è·å–å·²åˆå§‹åŒ–çš„ Provider å®ä¾‹ (å•ä¾‹)
        try:
            provider = await context.resources.get_instance(config.model)
        except Exception as e:
            raise ValueError(f"Failed to load model resource '{config.model}': {e}")

        # 3. [æ¸²æŸ“] Prompt
        system_instruction = config.system_prompt
        
        # JSON Schema æ³¨å…¥
        if config.response_format == "json_object" and config.output_definitions:
            try:
                target_schema = self._build_json_schema(config.output_definitions)
                json_instruction = f"""
                \n\n## Output Requirement
                You MUST respond with a valid JSON object strictly adhering to the following Schema.
                Output raw JSON only. Do not use Markdown blocks.
                
                JSON Schema:
                {json.dumps(target_schema, indent=2)}
                """
                system_instruction += json_instruction
            except Exception as e:
                logger.warning(f"Failed to build JSON schema: {e}")

        # æ¸²æŸ“å˜é‡
        system_content = TemplateRenderer.render(system_instruction, inputs)
        user_content = TemplateRenderer.render(config.prompt, inputs)
        
        # åˆå§‹åŒ–æ¶ˆæ¯å†å²
        # æ³¨æ„ï¼šPrompt ä¸åŒ…å«åœ¨ messages åˆ—è¡¨ä¸­ï¼Œè€Œæ˜¯ä½œä¸º system/user å‚æ•°ä¼ ç»™ Provider
        # ä½†ä¸ºäº† ReAct å¾ªç¯ï¼Œæˆ‘ä»¬éœ€è¦ç»´æŠ¤ä¸€ä¸ªæœ¬åœ°çš„ messages åˆ—è¡¨
        current_messages = [Message.user(user_content)]

        # 4. [æ‰§è¡Œ] ReAct Loop
        current_iter = 0
        final_response_content = ""
        final_reasoning_content = ""
        
        while current_iter < config.max_iterations:
            current_iter += 1
            
            # --- Stream Loop ---
            accumulated_text = ""
            current_tool_msg: Optional[Message] = None
            
            # ä½¿ç”¨ provider.stream è·å–æ‰“å­—æœºæ•ˆæœ
            # ä¼ é€’ tools å‚æ•°ï¼šå¦‚æœæ˜¯ç©ºåˆ—è¡¨ï¼Œä¼  Noneï¼Œæˆ–è€…å–å†³äº Provider å®ç°
            # ä¹‹å‰çš„ OpenAIProvider ä¿®å¤ç‰ˆæ”¯æŒä¼ ç©ºåˆ—è¡¨ï¼Œè¿™é‡Œä¼  openai_tools or None æœ€ç¨³å¦¥
            async for partial_msg, usage in provider.stream(
                system=system_content,
                messages=current_messages, # ä¼ é€’å½“å‰å†å²ï¼ˆä¸å« systemï¼‰
                tools=openai_tools or None
            ):
                if partial_msg:
                    # Case A: æ–‡æœ¬æµ
                    if partial_msg.content and isinstance(partial_msg.content[0], TextContent):
                        text_chunk = partial_msg.content[0].text
                        accumulated_text += text_chunk
                        # [Core] æ¨é€æµå¼ Token åˆ° EventBus
                        await context.streamer.emit(
                            SystemEvents.STREAM_TOKEN, 
                            text_chunk, 
                            producer_id=config.id
                        )
                    
                    # Case B: å·¥å…·è°ƒç”¨æ¶ˆæ¯ (é€šå¸¸åœ¨æµç»“æŸæ—¶ç”± Provider ç»„è£…å¥½è¿”å›)
                    # æ ¹æ®ä½ çš„ OpenAIProvider å®ç°ï¼Œå«æœ‰ tool_calls çš„ message ä¼šä½œä¸º partial_msg è¿”å›
                    if partial_msg.tool_calls:
                        current_tool_msg = partial_msg

                # Usage æš‚æ—¶å¿½ç•¥ï¼Œæˆ–è€…ç´¯åŠ 

            # Stream ç»“æŸï¼Œå¤„ç†ç»“æœ
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨
            if current_tool_msg and current_tool_msg.tool_calls:
                # å°† Assistant çš„å·¥å…·è°ƒç”¨æ¶ˆæ¯åŠ å…¥å†å²
                current_messages.append(current_tool_msg)
                
                logger.info(f"ğŸ”§ Tool Calls detected: {len(current_tool_msg.tool_calls)}")
                
                # æ‰§è¡Œæ‰€æœ‰å·¥å…·
                for tool_call_req in current_tool_msg.tool_calls:
                    # è§£åŒ… Request
                    # ToolRequest(id=..., toolCall=Result(value=CallToolRequestParam(...)))
                    if tool_call_req.tool_call.is_error:
                        continue
                        
                    param = tool_call_req.tool_call.value
                    call_id = tool_call_req.id
                    func_name = param.name
                    args = param.arguments

                    tool_result_content = ""
                    
                    # æŸ¥æ‰¾æœ¬åœ°å·¥å…·å®šä¹‰
                    target_tool = next((t for t in tool_defs if t.name == func_name), None)
                    
                    if target_tool:
                        try:
                            # æ‰§è¡Œå·¥å…· (æ”¯æŒ Sync å’Œ Async)
                            if target_tool.source_type == ToolSourceType.BUILTIN:
                                if getattr(target_tool, 'func', None):
                                    # [Core] æ³¨å…¥ context (å¦‚æœå·¥å…·å‡½æ•°éœ€è¦)
                                    # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å‚æ•°æ£€æµ‹ï¼Œæˆ–è€…çº¦å®šå·¥å…·å‡½æ•°ç­¾å
                                    # ç®€å•èµ·è§ï¼Œç›´æ¥ä¼  args
                                    res = target_tool.func(**args)
                                    if hasattr(res, '__await__'): 
                                        res = await res
                                    tool_result_content = json.dumps(res, ensure_ascii=False) if isinstance(res, (dict, list)) else str(res)
                            else:
                                tool_result_content = "Plugin tools not implemented yet"
                        except Exception as e:
                            tool_result_content = f"Error executing tool: {str(e)}"
                    else:
                        tool_result_content = f"Error: Tool {func_name} not found."

                    # å°†å·¥å…·ç»“æœå›å¡«ç»™ LLM (ä½œä¸º Tool Message)
                    current_messages.append(Message.tool(tool_result_content, tool_call_id=call_id))
                
                # ç»§ç»­ä¸‹ä¸€è½®å¾ªç¯ (Chat with Tool Results)
                continue

            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿™æ˜¯æœ€ç»ˆå›å¤
                # å°†çº¯æ–‡æœ¬å›å¤åŠ å…¥å†å² (ä¿æŒå®Œæ•´æ€§)
                assistant_msg = Message.assistant(accumulated_text)
                current_messages.append(assistant_msg)
                
                final_response_content = accumulated_text
                break
        
        # 5. [è§£æ] ç»“æœå¤„ç†
        final_output = {}
        
        # æ¨¡å¼ A: JSON Object
        if config.response_format == "json_object":
            try:
                cleaned_json = self._clean_json_markdown(final_response_content)
                parsed_data = json.loads(cleaned_json)
                final_output = parsed_data
            except Exception as e:
                logger.error(f"JSON Parse Error: {e}")
                final_output = {"output": final_response_content, "_error": "JSON parse failed"}
        
        # æ¨¡å¼ B: Text
        else:
            # æ™ºèƒ½æ˜ å°„ï¼šå¦‚æœå‰ç«¯å®šä¹‰äº†è¾“å‡ºå˜é‡åï¼Œå°è¯•å°†ç»“æœèµ‹ç»™ç¬¬ä¸€ä¸ªå˜é‡
            output_key = "output"
            if config.output_definitions:
                valid_defs = [d for d in config.output_definitions if d.name not in ["reasoning_content"]]
                if valid_defs:
                    output_key = valid_defs[0].name
            
            final_output[output_key] = final_response_content

        return final_output

    # --- Helpers ---

    def _build_json_schema(self, output_defs: List[OutputDefinition]) -> Dict[str, Any]:
        """æ„å»º JSON Schema"""
        if not output_defs: return {}
        
        properties = {}
        required = []
        
        for item in output_defs:
            schema_type = item.type if item.type != "json" else "object"
            prop = {"type": schema_type}
            
            if schema_type == "array":
                prop["items"] = {"type": "string"}
            if schema_type == "object":
                prop["additionalProperties"] = True
            if item.description:
                prop["description"] = item.description
                
            properties[item.name] = prop
            required.append(item.name)
            
        return {
            "type": "object",
            "properties": properties,
            "required": required,
            "additionalProperties": False
        }

    def _clean_json_markdown(self, text: str) -> str:
        text = text.strip()
        pattern = r"^```(?:json)?\s*(\{.*?\})\s*```$"
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1)
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1:
            return text[start : end + 1]
        return text

    def _to_openai_tool(self, tool_def: ToolDefinition) -> Dict:
        return {
            "type": "function",
            "function": {
                "name": tool_def.name,
                "description": tool_def.description or "",
                "parameters": tool_def.args_schema or {"type": "object", "properties": {}}
            }
        }