import asyncio
import uuid
import logging
from typing import Dict, Any, AsyncGenerator,List,Optional
import json
# Core Modules
import goose.globals as G
from goose.workflow.graph import Graph
from goose.workflow.scheduler import WorkflowScheduler
from goose.workflow.converter import WorkflowConverter
from goose.adapter import AdapterManager
from goose.workflow import WorkflowDefinition, WorkflowRepository
from goose.session.hook import SessionPersistenceHook
from goose.globals import get_runtime

# Repositories
from goose.app.user.repository import UserResourceRepository
from .repository import ExecutionRepository

logger = logging.getLogger("goose.app.execution")


class ExecutionService:
    def __init__(self, 
                 converter: WorkflowConverter,
                 wf_repo: WorkflowRepository,
                 exec_repo: ExecutionRepository,
                 auth_repo: UserResourceRepository):
        self.wf_repo = wf_repo
        self.exec_repo = exec_repo
        self.converter = converter
        self.auth_repo = auth_repo


        
    async def get_execution(self, run_id: str) -> Dict[str, Any]:
        """èŽ·å–è¯¦æƒ…"""
        res = await self.exec_repo.get(run_id)
        if not res:
            raise ValueError("Execution not found")
        return res

    async def list_executions(self, wf_id: str, page: int, size: int) -> List[Dict[str, Any]]:
        """èŽ·å–åŽ†å²åˆ—è¡¨"""
        offset = (page - 1) * size
        return await self.exec_repo.list(wf_id, size, offset)
    
    async def resume_workflow(self, run_id: str, inputs: Dict[str, Any] = None) -> None:
        """
        [ä¸šåŠ¡é€»è¾‘] æ¢å¤æš‚åœ/å¤±è´¥çš„ä»»åŠ¡
        """
        # 1. æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        exec_record = await self.exec_repo.get(run_id)
        if not exec_record:
            raise ValueError(f"Execution {run_id} not found")

        wf_id = exec_record["workflow_id"]
        graph = await self._prepare_run(wf_id)
        
        # 2. æ›´æ–°çŠ¶æ€ (Optional: å¦‚æžœä¼ å…¥äº†æ–° inputsï¼Œå¯èƒ½éœ€è¦åˆå¹¶åˆ° Context)
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œinputs ä»…ç”¨äºŽæ›´æ–° Contextï¼Œå…·ä½“ç”± Scheduler å¤„ç†
        
        # 3. å¯åŠ¨è°ƒåº¦å™¨ (Resume Mode)
        session_hook = SessionPersistenceHook()
        scheduler = WorkflowScheduler(hooks=[session_hook])
        
        # è¿™é‡Œçš„å…³é”®æ˜¯ resume=True
        asyncio.create_task(scheduler.run(
            graph=graph, 
            inputs=inputs or {}, # è¿™é‡Œä¼ å…¥çš„ inputs ä¼šåˆå¹¶åˆ° context
            run_id=run_id, 
            resume=True
        ))
        
        logger.info(f"ðŸ”„ Execution resumed: {run_id}")

    async def test_single_node(self, node_type: str, config: Dict, inputs: Dict, mock_ctx: Dict) -> Any:
        """
        [è°ƒè¯•é€»è¾‘] è¿è¡Œå•ä¸ªèŠ‚ç‚¹ï¼Œä¸æ¶‰åŠå·¥ä½œæµæŒä¹…åŒ–
        """
        runtime = get_runtime()
        
        # 1. å·¥åŽ‚åˆ›å»ºç»„ä»¶å®žä¾‹
        # å‡è®¾ runtime æœ‰ component_factory (æˆ–è€… resource_manager)
        # è¿™é‡Œæ¼”ç¤ºæ‰‹åŠ¨ä»Ž ResourceKind/Registry åŠ è½½
        # å®žé™…ä»£ç å¯èƒ½ï¼šcomponent = runtime.component_factory.create(node_type)
        from goose.resources.factory import create_component_by_type # å‡è®¾ä½ æœ‰è¿™ä¸ªå·¥åŽ‚æ–¹æ³•
        component = create_component_by_type(node_type)
        
        if not component:
            raise ValueError(f"Unknown node type: {node_type}")

        # 2. æž„å»ºä¸´æ—¶ä¸Šä¸‹æ–‡
        temp_run_id = f"test_{uuid.uuid4().hex[:6]}"
        context = WorkflowContext(
            session_id=temp_run_id,
            variables=mock_ctx
        )
        # æ³¨å…¥ä¾èµ– (Resource, etc.)
        context.set_services(resources=get_resource_manager(), streamer=None, executor=None)

        # 3. æ‰§è¡Œ
        # åŠ ä¸Š config['id'] é¿å…ç»„ä»¶æŠ¥é”™
        if "id" not in config: config["id"] = "test_node"
        
        output = await component.invoke(inputs, config, context)
        return output

    async def get_event_generator(self, run_id: str, last_event_id: int = -1) -> AsyncGenerator:
        """
        [Stream Logic] èŽ·å–äº‹ä»¶æµ
        æ”¯æŒï¼šåŽ†å²å›žå¡« (Backfill) + å®žæ—¶ç›‘å¬ (Realtime)
        """
        # 1. éªŒè¯ run_id å­˜åœ¨
        if not await self.exec_repo.get(run_id):
            raise ValueError("Execution not found")

        runtime = get_runtime()
        streamer = runtime.streamer_factory.create(run_id)
        
        # Streamer.listen å†…éƒ¨å°è£…äº† "å…ˆæŸ¥ DB events è¡¨ï¼Œå†ç›‘å¬ Bus" çš„é€»è¾‘
        async for event in streamer.listen(after_seq_id=last_event_id):
            yield event

    async def get_execution_detail(self, run_id: str) -> Dict:
        data = await self.exec_repo.get(run_id)
        if not data:
            raise ValueError("Execution not found")
        return data
    
    
    
    
    # ==========================================
    # 1. æ ¸å¿ƒï¼šæµå¼æ‰§è¡Œ (è§£å†³ç«žæ€æ¡ä»¶)
    # ==========================================
    
    async def execute_stream_generator(
        self, 
        wf_id: str, 
        inputs: Dict[str, Any], 
        user_id: str
    ) -> AsyncGenerator[Dict, None]:
        """
        [SSE å…¥å£] åˆ›å»ºä»»åŠ¡å¹¶è¿”å›žäº‹ä»¶æµ
        é‡‡ç”¨ Queue ç¼“å†²æ¨¡å¼ï¼Œç¡®ä¿åœ¨ä»»åŠ¡å¯åŠ¨å‰ç›‘å¬å™¨å·²å°±ç»ªã€‚
        """
        # 1. é‰´æƒä¸Žåˆå§‹åŒ–è®°å½•
        run_id = await self._create_execution_record(wf_id, inputs, user_id)
        
        # 2. å‡†å¤‡ç»„ä»¶
        runtime = get_runtime()
        streamer = runtime.streamer_factory.create(run_id)
        
        # 3. åˆ›å»ºç¼“å†²é˜Ÿåˆ— (æ ¸å¿ƒä¼˜åŒ–ç‚¹)
        # ä½œç”¨ï¼šä½œä¸º EventBus å’Œ HTTP Response ä¹‹é—´çš„æ¡¥æ¢
        event_queue = asyncio.Queue()

        # 4. å®šä¹‰åŽå°æ‰§è¡Œä»»åŠ¡
        async def background_runner():
            try:
                # çœŸæ­£çš„ä¸šåŠ¡é€»è¾‘æ‰§è¡Œ
                await self._run_scheduler_task(run_id, wf_id, inputs, user_id, streamer)
            except Exception as e:
                logger.error(f"Background runner failed: {e}", exc_info=True)
                await streamer.emit("error", {"error": str(e)})
            finally:
                # ä»»åŠ¡ç»“æŸï¼ˆæ— è®ºæˆåŠŸå¤±è´¥ï¼‰ï¼Œç»™é˜Ÿåˆ—å‘ä¸€ä¸ª None å“¨å…µï¼Œé€šçŸ¥æ¶ˆè´¹è€…åœæ­¢
                await event_queue.put(None)

        # 5. å®šä¹‰ç›‘å¬ä»»åŠ¡
        # ä½œç”¨ï¼šæŠŠ Streamer æ”¶åˆ°çš„æ¶ˆæ¯æ¬è¿åˆ° Queue é‡Œ
        async def event_listener():
            async for event in streamer.listen():
                await event_queue.put(event)
        
        # 6. å¯åŠ¨åŒä»»åŠ¡
        # å…³é”®ï¼šå…ˆå¯åŠ¨ç›‘å¬ï¼Œå†å¯åŠ¨æ‰§è¡Œã€‚è™½ç„¶æ˜¯å¹¶å‘ï¼Œä½† Queue ä¿è¯äº†æ¶ˆæ¯ä¸ä¼šä¸¢ã€‚
        listener_task = asyncio.create_task(event_listener())
        runner_task = asyncio.create_task(background_runner())

        # 7. æ¶ˆè´¹é˜Ÿåˆ— (HTTP å“åº”ç”Ÿæˆå™¨)
        try:
            while True:
                # ç­‰å¾…é˜Ÿåˆ—æ¶ˆæ¯
                event = await event_queue.get()
                
                # æ”¶åˆ°å“¨å…µ Noneï¼Œè¯´æ˜Žä»»åŠ¡ç»“æŸ
                if event is None:
                    break
                
                # æ ¼å¼åŒ–å¹¶ Yield ç»™å‰ç«¯
                # å¦‚æžœ event æ˜¯ Pydantic å¯¹è±¡ï¼Œè½¬ dict
                data = event.dict() if hasattr(event, "dict") else event
                yield data
                
                # å¦‚æžœæ˜¯ç»ˆæ­¢äº‹ä»¶ï¼Œå¯ä»¥æå‰è·³å‡ºï¼ˆåŒé‡ä¿é™©ï¼‰
                event_type = data.get("type")
                if event_type in ["workflow_completed", "error", "workflow_failed"]:
                    break
                    
        except asyncio.CancelledError:
            logger.warning(f"Client disconnected stream {run_id}")
            # å®¢æˆ·ç«¯æ–­å¼€è¿žæŽ¥ï¼Œå–æ¶ˆåŽå°ä»»åŠ¡ (å¯é€‰ï¼Œè§†ä¸šåŠ¡éœ€æ±‚è€Œå®š)
            # runner_task.cancel()
            raise

    
    # ==========================================
    # 2. æ™®é€šå¼‚æ­¥æ‰§è¡Œ (Fire & Forget)
    # ==========================================

    async def run_workflow(self, wf_id: str, inputs: Dict[str, Any], user_id: str) -> str:
        """
        [API å…¥å£] ä»…è§¦å‘ä»»åŠ¡ï¼Œç«‹å³è¿”å›ž ID
        """
        # 1. åˆå§‹åŒ–
        run_id = await self._create_execution_record(wf_id, inputs, user_id)
        
        # 2. èŽ·å– Runtime Streamer (å³ä½¿ä¸æµå¼è¾“å‡ºï¼ŒScheduler å†…éƒ¨ä¹Ÿéœ€è¦å®ƒæ¥å‘äº‹ä»¶)
        runtime = get_runtime()
        streamer = runtime.streamer_factory.create(run_id)

        # 3. ä¸¢è¿›åŽå°è¿è¡Œ
        asyncio.create_task(
            self._run_scheduler_task(run_id, wf_id, inputs, user_id, streamer)
        )
        
        return run_id
    
    # ==========================================
    # 3. å†…éƒ¨æ ¸å¿ƒé€»è¾‘ (åŽŸå­åŒ–å°è£…)
    # ==========================================

    async def _create_execution_record(self, wf_id: str, inputs: Dict, user_id: str) -> str:
        """Helper: é‰´æƒå¹¶åˆ›å»º DB è®°å½•"""
        # A. é‰´æƒ
        if not await self.auth_repo.check_ownership(user_id, wf_id):
            raise ValueError(f"Permission denied: User {user_id} cannot access workflow {wf_id}")

        # B. åˆ›å»º ID
        run_id = f"run_{uuid.uuid4().hex}"
        
        # C. å­˜åº“
        await self.exec_repo.create(run_id, wf_id, inputs)
        await self.auth_repo.bind(user_id, run_id, "execution")
        
        return run_id

    async def _run_scheduler_task(
        self, 
        run_id: str, 
        wf_id: str, 
        inputs: Dict, 
        user_id: str,
        streamer
    ):
        """
        [Heavy Lifting] çœŸæ­£çš„è°ƒåº¦é€»è¾‘
        åŒ…å«ï¼šçŠ¶æ€æ›´æ–°ã€èµ„æºåŠ è½½ã€å›¾æž„å»ºã€è¿è¡Œã€ç»“æžœä¿å­˜
        """
        runtime = get_runtime()
        
        try:
            # 1. æ›´æ–°çŠ¶æ€ -> Running
            await self.exec_repo.update_status(run_id, "running")
            
            # 2. å‡†å¤‡ Graph
            # è¿™é‡Œéœ€è¦ä»Ž WorkflowRepo åŠ è½½å®šä¹‰ï¼Œè½¬æ¢æˆ Graph å¯¹è±¡
            # å‡è®¾ä½ æœ‰ä¸€ä¸ª helper æ–¹æ³•åšè¿™ä¸ªäº‹
            graph = await self._load_and_build_graph(wf_id)
            
            # 3. å‡†å¤‡èµ„æºç®¡ç†å™¨ (æ³¨å…¥ç”¨æˆ· API Key)
            resource_manager = runtime.create_resource_manager(user_id)
            
            # 4. åˆå§‹åŒ–è°ƒåº¦å™¨
            # SessionHook ç”¨äºŽèŠ‚ç‚¹æ‰§è¡Œå®ŒåŽä¿å­˜ä¸­é—´çŠ¶æ€
            scheduler = WorkflowScheduler(hooks=[SessionPersistenceHook()])
            
            logger.info(f"ðŸš€ Scheduler starting for {run_id}")
            
            # 5. æ‰§è¡Œ (Await until finish)
            output = await scheduler.run(
                graph=graph,
                inputs=inputs,
                run_id=run_id,
                resource_manager=resource_manager,
                streamer=streamer # ä¼ å…¥ Streamer ä¾›èŠ‚ç‚¹å‘é€ token
            )
            
            # 6. å¤„ç†ç»“æžœ
            # åˆ¤æ–­æ˜¯æš‚åœ(suspended)è¿˜æ˜¯å®Œæˆ(completed)
            final_status = "completed"
            if isinstance(output, dict) and output.get("status") == "suspended":
                final_status = "suspended"
            
            # 7. æ›´æ–° DB -> Completed
            # æ³¨æ„ï¼šoutputs æœ€å¥½è½¬æˆ JSON å­—ç¬¦ä¸²å­˜åº“
            await self.exec_repo.update_status(
                run_id, 
                final_status, 
                result=output
            )
            logger.info(f"âœ… Scheduler finished for {run_id}: {final_status}")

        except Exception as e:
            logger.error(f"âŒ Scheduler failed for {run_id}: {e}", exc_info=True)
            # æ›´æ–° DB -> Failed
            await self.exec_repo.update_status(run_id, "failed", error=str(e))
            # ç¡®ä¿é”™è¯¯ä¹Ÿèƒ½é€šè¿‡ SSE å‘å‡ºåŽ»
            await streamer.emit("error", {"error": str(e)})
            raise e

    # ==========================================
    # 4. è¾…åŠ©æ–¹æ³•
    # ==========================================

    async def _load_and_build_graph(self, wf_id: str) -> Graph:
        """åŠ è½½å·¥ä½œæµå®šä¹‰å¹¶è½¬æ¢ä¸ºå›¾å¯¹è±¡"""
        
        wf_def = await self.wf_repo.get(wf_id)
        if not wf_def:
            raise ValueError(f"Workflow {wf_id} not found")
        
        # Definition -> Graph
        return self.converter.convert(wf_def)

