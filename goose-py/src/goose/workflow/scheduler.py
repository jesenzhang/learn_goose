import logging
import asyncio
from typing import Any, AsyncGenerator, Optional, Dict, Union, List

# å¼•ç”¨æ ¸å¿ƒä¾èµ–
from goose.workflow.protocol import ControlSignal
from goose.workflow.graph import Graph
from goose.workflow.context import WorkflowContext
from goose.workflow.events import (
    WorkflowEvent, WorkflowEventType, 
    NodeEvent, NodeFinishedEvent, WorkflowCompletedEvent
)
from goose.workflow.persistence import WorkflowState, WorkflowCheckpointer
from goose.session import SessionManager, SessionType
from goose.workflow.repository import WorkflowRepository, register_workflow_schemas

from goose.globals import get_streamer_factory, get_runtime

logger = logging.getLogger("goose.workflow.scheduler")

class WorkflowScheduler:
    """
    [Core] å·¥ä½œæµè°ƒåº¦å¼•æ“ã€‚
    è´Ÿè´£å›¾çš„éå†ã€èŠ‚ç‚¹æ‰§è¡Œã€çŠ¶æ€ç®¡ç†ã€æŒä¹…åŒ–å’Œäº‹ä»¶åˆ†å‘ã€‚
    """
    def __init__(self, graph: Graph, checkpointer: Optional[WorkflowCheckpointer] = None):
        self.graph = graph
        # ç¡®ä¿æ•°æ®åº“ Schema å·²å°±ç»ª
        register_workflow_schemas()
        # é»˜è®¤ä½¿ç”¨åŸºäº SQLite çš„ Repository
        self.checkpointer = checkpointer or WorkflowRepository()

    async def run(
        self, 
        input_data: Any, 
        run_id: str = None, 
        resume: bool = False,
        parent_ctx: WorkflowContext = None,
        resource_manager: Optional['ResourceManager'] = None
    ) -> Any:
        """
        æ‰§è¡Œå·¥ä½œæµã€‚
        :param input_data: åˆå§‹è¾“å…¥æ•°æ®
        :param run_id: ä¼šè¯ ID (Session ID)
        :param resume: æ˜¯å¦ä»æ–­ç‚¹æ¢å¤
        :param parent_ctx: çˆ¶çº§ä¸Šä¸‹æ–‡ (ç”¨äºå­å·¥ä½œæµå˜é‡ç»§æ‰¿)
        """
        # 1. æ‰¾åˆ°å…¥å£ ID
        entry_point_id = self.graph.entry_point
        if not entry_point_id:
             raise ValueError("Graph has no entry point!")
         
        runtime = get_runtime()
        # ==========================================
        # 1. åˆå§‹åŒ– Session & Context
        # ==========================================
        should_inject_start = False
        
        if not run_id:
            # è‡ªåŠ¨åˆ›å»ºæ¨¡å¼
            session = await SessionManager.create_workflow_session(name="Auto Workflow Run")
            run_id = session.id
            should_inject_start = True
            logger.info(f"ğŸ†• Auto-created Workflow Session: {run_id}")
        else:
            # æŒ‡å®š ID æ¨¡å¼ (Resume æˆ– å­å›¾)
            try:
                await SessionManager.get_session(run_id)
                if resume:
                    logger.info(f"ğŸ”„ Resuming session {run_id}")
                    should_inject_start = False
                else:
                    logger.info(f"ğŸ”„ Restarting session {run_id}")
                    should_inject_start = True
            except ValueError:
                # Session ä¸å­˜åœ¨ï¼Œå¼ºåˆ¶åˆ›å»º (é€šå¸¸ç”¨äºå­å›¾)
                await SessionManager.create_session(
                    session_id=run_id, 
                    name=f"Sub-Workflow {run_id[-6:]}", 
                    session_type=SessionType.WORKFLOW
                )
                should_inject_start = True

        # B. è·å– Streamer
        streamer = runtime.streamer_factory.create(run_id)
        # C. å…œåº• Resource Manager (é˜²æ­¢è°ƒç”¨æ–¹æœªä¼ )
        if resource_manager is None:
            logger.warning("âš ï¸ No ResourceManager provided. Creating default (system-only).")
            resource_manager = runtime.create_resource_manager(user_id=None)
        
        # ==========================================
        # 2. ä¸Šä¸‹æ–‡æ„å»ºä¸æ³¨å…¥
        # ==========================================
        
        # åˆå§‹å˜é‡ (ç”¨äº ValueResolver è§£æå…¨å±€å˜é‡ {{ var }})
        initial_vars = input_data if isinstance(input_data, dict) else {"input": input_data}
        
        context = WorkflowContext(
            run_id=run_id,
            parent_run_id=parent_ctx.run_id if parent_ctx else None,
            variables=initial_vars
        )
        # [Feature] å˜é‡ç»§æ‰¿: å°†çˆ¶çº§ä¸Šä¸‹æ–‡å˜é‡å¤åˆ¶åˆ°å½“å‰ä¸Šä¸‹æ–‡
        if parent_ctx:
            context.variables.update(parent_ctx.variables)
            
        context.set_services(
            resources=resource_manager,
            streamer=streamer,
            executor=self
        )
        
        # ==========================================
        # 2. çŠ¶æ€æ¢å¤ä¸é˜Ÿåˆ—åˆå§‹åŒ–
        # ==========================================
        queue = [] # BFS æ‰§è¡Œé˜Ÿåˆ—
        
        if resume and self.checkpointer:
            state = await self.checkpointer.load_checkpoint(run_id)
            if state and state.status not in ["completed", "failed", "cancelled"]:
                logger.info(f"ğŸ“¥ Checkpoint loaded. Resuming from node: {state.current_node_id}")
                # æ¢å¤ä¸Šä¸‹æ–‡æ•°æ®
                context.node_outputs = state.context_data
                
                # æ¢å¤æ‰§è¡Œé˜Ÿåˆ—
                # æ³¨æ„: å½“å‰æ¶æ„ WorkflowState ä»…å­˜å‚¨å•ä¸ª current_node_id
                # è¿™æ„å‘³ç€å¦‚æœå´©æºƒæ—¶é˜Ÿåˆ—ä¸­æœ‰å¤šä¸ªå¹¶è¡ŒèŠ‚ç‚¹ï¼Œåªèƒ½æ¢å¤å¤´éƒ¨çš„ä¸€ä¸ª
                # TODO: å‡çº§ WorkflowState æ”¯æŒ queue: List[str] ä»¥æ”¯æŒå®Œç¾å¹¶è¡Œæ¢å¤
                if state.execution_queue:
                    queue.extend(state.execution_queue)
                    logger.info(f"ğŸ“¥ Resuming {len(queue)} nodes: {queue}")
                else:
                    # å…¼å®¹æ€§ï¼šå¦‚æœçŠ¶æ€æ˜¯ running ä½†é˜Ÿåˆ—ä¸ºç©ºï¼Œå¯èƒ½æ˜¯æ—§æ•°æ®æˆ–å¼‚å¸¸
                    logger.warning("âš ï¸ Resuming running state but queue is empty.")
                
                should_inject_start = False
            else:
                logger.warning(f"âš ï¸ Checkpoint invalid or completed. Restarting from scratch.")
                should_inject_start = True

        # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼ˆæ–°è¿è¡Œæˆ–æ¢å¤å¤±è´¥ï¼‰ï¼Œä» Entry Point å¼€å§‹
        if not queue:
            if self.graph.entry_point:
                queue.append(self.graph.entry_point)
            else:
                logger.warning("ğŸš« No entry point found in Graph. Workflow might be empty.")

        
        # å‘é€å¼€å§‹äº‹ä»¶
        await streamer.emit(type=WorkflowEventType.WORKFLOW_STARTED, data=run_id)

        try:
            # ==========================================
            # 3. æ‰§è¡Œä¸»å¾ªç¯ (Execution Loop)
            # ==========================================
            while queue:
                current_node_id = queue.pop(0)
                
                # --- A. æŒ‚èµ·æ§åˆ¶ ---
                if current_node_id == "__SUSPEND__":
                    logger.info(f"â¸ï¸ Workflow {run_id} suspended.")
                    await self._save_state(run_id, current_node_id, context, "suspended")
                    return

                # --- B. è·å–èŠ‚ç‚¹å®ä¾‹ ---
                node = self.graph.get_node(current_node_id)
                if not node:
                    logger.error(f"âŒ Node {current_node_id} not found in graph definition.")
                    continue
                
                node_type = getattr(node, "name", node.__class__.__name__)

                # --- C. äº‹ä»¶: Node Started ---
                yield NodeEvent(
                    type=WorkflowEventType.NODE_STARTED,
                    session_id=run_id,
                    node_id=current_node_id,
                    node_type=node_type,
                    input_data="" # ç®€åŒ–æ—¥å¿—ï¼Œå…·ä½“æ•°æ®åœ¨ Context ä¸­
                )

                # --- D. æ‰§è¡ŒèŠ‚ç‚¹ (Invoke) ---
                # Start èŠ‚ç‚¹ç‰¹æ®Šå¤„ç†ï¼šä¼ å…¥ input_data
                # å…¶ä»–èŠ‚ç‚¹ï¼šä¼ å…¥ None (ä¾èµ–å†…éƒ¨ resolve_inputs ä» context è·å–)
                node_input = input_data if current_node_id == self.graph.entry_point else None
                node_config = node.config
                
                try:
                    # [Core] è°ƒç”¨ç»„ä»¶é€»è¾‘
                    output = await node.invoke(node_input,node_config, context)
                except Exception as e:
                    logger.error(f"âŒ Node {current_node_id} execution failed: {e}", exc_info=True)
                    # å¯ä»¥åœ¨è¿™é‡Œå†³å®šæ˜¯ Fail-Fast è¿˜æ˜¯ Fail-Soft
                    raise e 

                # --- E. æ›´æ–°ä¸Šä¸‹æ–‡ ---
                context.set_node_output(current_node_id, output)

                # --- F. äº‹ä»¶: Node Finished ---
                yield NodeFinishedEvent(
                    type=WorkflowEventType.NODE_FINISHED, # [Fix] å¿…å¡«å­—æ®µ
                    session_id=run_id,
                    node_id=current_node_id,
                    node_type=node_type,
                    output_data=output
                )

                # --- G. è·¯ç”±ä¸æ§åˆ¶æµ (Routing) ---
                
                # 1. ä¿¡å·æ‹¦æˆª (Break/Continue)
                if isinstance(output, dict) and ControlSignal.SIGNAL_KEY in output:
                    signal = output[ControlSignal.SIGNAL_KEY]
                    logger.info(f"ğŸš¦ Control Signal received: {signal} at {current_node_id}")
                    # ä¿¡å·ä¸å†å‘ä¸‹æ¸¸ä¼ æ’­ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯(å…¶å®æ˜¯è·³è¿‡åç»­å…¥é˜Ÿ)ï¼Œ
                    # ç­‰å¾… Loop ç»„ä»¶æ•è·æ­¤ Event
                    continue

                # 2. ç¡®å®šä¸‹æ¸¸èŠ‚ç‚¹
                outgoing_edges = self.graph.get_outgoing_edges(current_node_id)
                next_nodes = []
                
                # 3. æ£€æŸ¥ Active Handle (Branching)
                active_handle = None
                if isinstance(output, dict):
                    active_handle = output.get(ControlSignal.ACTIVE_HANDLE)

                if active_handle:
                    # åˆ†æ”¯æ¨¡å¼: åªæ¿€æ´»åŒ¹é…çš„è¾¹
                    logger.info(f"ğŸ”€ Branching: {current_node_id} -> Handle '{active_handle}'")
                    for edge in outgoing_edges:
                        if edge.source_handle == active_handle:
                            next_nodes.append(edge.target)
                else:
                    # é»˜è®¤æ¨¡å¼: æ¿€æ´»æ‰€æœ‰é»˜è®¤è¾¹ (source_handle is None)
                    for edge in outgoing_edges:
                        if edge.source_handle is None:
                            next_nodes.append(edge.target)

                # 4. å…¥é˜Ÿ
                for nid in next_nodes:
                    # ç®€å• DAG å»é‡ (é˜²æ­¢è±å½¢ç»“æ„é‡å¤æ‰§è¡Œ)
                    # å¤æ‚å¾ªç¯å›¾éœ€é…åˆ visit count
                    queue.append(nid)

                # --- H. æŒä¹…åŒ– (Checkpoint) ---
                # ä¿å­˜"ä¸‹ä¸€æ­¥è¦åšä»€ä¹ˆ"
                status_to_save = "running" if queue else "completed"
                # next_node_to_save = queue[0] if queue else "completed"
                # await self._save_state(run_id, next_node_to_save, context, "running")
                await self._save_state(run_id, queue, context, status_to_save)

            # ==========================================
            # 4. æµç¨‹ç»“æŸ
            # ==========================================
            logger.info(f"ğŸ Workflow {run_id} Execution Loop Finished.")
            
            # [Fix] æ˜¾å¼ä¿å­˜æœ€ç»ˆ Completed çŠ¶æ€
            await self._save_state(run_id, [], context, "completed")
            
            # æå–æœ€ç»ˆè¾“å‡º (Heuristic: ä¼˜å…ˆæ‰¾ End èŠ‚ç‚¹ï¼Œå¦åˆ™æ‰¾æœ€åä¸€ä¸ª)
            final_output = {}
            for nid, out in context.node_outputs.items():
                final_output = out # ç®€å•å–æœ€åä¸€ä¸ª
                # å¦‚æœæœ‰ä¸“é—¨çš„ End èŠ‚ç‚¹é€»è¾‘å¯åœ¨æ­¤åŠ å¼º
            
            yield WorkflowCompletedEvent(
                session_id=run_id,
                final_output=final_output
            )

        except Exception as e:
            logger.error(f"ğŸ’¥ Workflow {run_id} Crashed: {e}", exc_info=True)
            yield WorkflowEvent(type=WorkflowEventType.WORKFLOW_ERROR, session_id=run_id, text=str(e))
            # ä¿å­˜å¤±è´¥çŠ¶æ€
            retry_queue = [current_node_id] + queue
            await self._save_state(run_id, retry_queue, context, "failed")
            raise e

    # ==========================================
    # Helpers
    # ==========================================

    async def _save_state(self, run_id: str, execution_queue: List[str], context: WorkflowContext, status: str):
        """æŒä¹…åŒ–çŠ¶æ€è¾…åŠ©æ–¹æ³•"""
        if self.checkpointer:
            await self.checkpointer.save_checkpoint(WorkflowState(
                run_id=run_id,
                execution_queue=execution_queue,
                context_data=context.node_outputs,
                status=status
            ))

    def _inject_start_data(self, context: WorkflowContext, input_data: Any):
        """å°†åˆå§‹è¾“å…¥æ³¨å…¥åˆ° 'start' èŠ‚ç‚¹çš„è¾“å‡ºä¸­ï¼Œä¾›åç»­èŠ‚ç‚¹å¼•ç”¨ {{ start.key }}"""
        if isinstance(input_data, dict):
            context.set_node_output("start", input_data)
        else:
            context.set_node_output("start", {"input": input_data})

    async def run_to_completion(
        self, 
        inputs: Dict[str, Any], 
        parent_ctx: Optional[WorkflowContext] = None
    ) -> Dict[str, Any]:
        """
        [Sync-like Helper] è¿è¡Œå­å›¾ç›´åˆ°ç»“æŸï¼Œå¹¶è¿”å›ç»“æœã€‚
        ä¾› LoopComponent / SubWorkflowComponent å†…éƒ¨è°ƒç”¨ã€‚
        
        :param inputs: å­å›¾è¾“å…¥
        :param parent_ctx: çˆ¶çº§ä¸Šä¸‹æ–‡ (å¿…é¡»ä¼ å…¥ï¼Œå¦åˆ™å­å›¾æ— æ³•è®¿é—®çˆ¶çº§å˜é‡)
        """
        # ç”Ÿæˆä¸´æ—¶ ID
        sub_run_id = f"sub_{uuid.uuid4().hex[:8]}"
        final_res = {}
        
        # è°ƒç”¨ runï¼Œä¼ å…¥ parent_ctx
        async for event in self.run(inputs, run_id=sub_run_id, parent_ctx=parent_ctx):
            
            # 1. æ•è·æœ€ç»ˆç»“æœ
            if event.type == WorkflowEventType.WORKFLOW_COMPLETED:
                if isinstance(event, WorkflowCompletedEvent):
                    final_res = event.final_output
            
            # 2. æ•è·æ§åˆ¶ä¿¡å· (Break/Continue) å¹¶ç«‹å³å‘ä¸Šå†’æ³¡
            if event.type == WorkflowEventType.NODE_FINISHED:
                if isinstance(event, NodeFinishedEvent):
                    data = event.output_data
                    if isinstance(data, dict) and ControlSignal.SIGNAL_KEY in data:
                        return data # ç«‹å³è¿”å›ä¿¡å·å­—å…¸
        
        return final_res